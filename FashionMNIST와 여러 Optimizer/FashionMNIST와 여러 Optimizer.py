# -*- coding: utf-8 -*-
"""DL_HW3_60192328_강건희.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Al4EbPV1SX09FsoXrrrRrSc3b-lssJN6

# **문제1번**
"""

import torch
from torch import nn
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torchvision import datasets
from torchvision.transforms import ToTensor

# 하이퍼파라미터
lr = 0.001
epochs = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1. 전체 데이터셋 불러오기 및 합치기
train_data_raw = datasets.FashionMNIST(root="data", train=True, download=True, transform=ToTensor())
test_data_raw = datasets.FashionMNIST(root="data", train=False, download=True, transform=ToTensor())
full_dataset = ConcatDataset([train_data_raw, test_data_raw])  # 전체 70,000개

# 2. 8:1:1로 나누기
total_size = len(full_dataset)  # 70,000
train_size = int(0.8 * total_size)     # 56,000
val_size = int(0.1 * total_size)       # 7,000
test_size = total_size - train_size - val_size  # 7,000
train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])

# 3. DataLoader 설정 (minibatch 사용 안함 → batch_size = 전체)
train_loader = DataLoader(train_data, batch_size=len(train_data))
val_loader = DataLoader(val_data, batch_size=len(val_data))
test_loader = DataLoader(test_data, batch_size=len(test_data))

# 4. MLP 모델 정의 (2 hidden layer)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model(x)

model = MLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=lr)

# 5. 학습 루프
for epoch in range(epochs):
    model.train()
    for X_train, y_train in train_loader:
        X_train, y_train = X_train.to(device), y_train.to(device)
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()

# 모든 에폭 끝난 후, train/validation/test 성능 평가
model.eval()
with torch.no_grad():
    # Train Accuracy 측정
    for X_train, y_train in train_loader:
        X_train, y_train = X_train.to(device), y_train.to(device)
        pred_train = model(X_train)
        train_loss = criterion(pred_train, y_train).item()
        train_acc = (pred_train.argmax(1) == y_train).float().mean().item()

    # Validation Accuracy 측정
    for X_val, y_val in val_loader:
        X_val, y_val = X_val.to(device), y_val.to(device)
        pred_val = model(X_val)
        val_loss = criterion(pred_val, y_val).item()
        val_acc = (pred_val.argmax(1) == y_val).float().mean().item()

    # Test Accuracy 측정
    for X_test, y_test in test_loader:
        X_test, y_test = X_test.to(device), y_test.to(device)
        pred_test = model(X_test)
        test_loss = criterion(pred_test, y_test).item()
        test_acc = (pred_test.argmax(1) == y_test).float().mean().item()

# 최종 출력
print("\n== 최종 결과 ==")
print(f"Train Accuracy:     {train_acc*100:.2f}% | Loss: {train_loss:.4f}")
print(f"Validation Accuracy:{val_acc*100:.2f}% | Loss: {val_loss:.4f}")
print(f"Test Accuracy:      {test_acc*100:.2f}% | Loss: {test_loss:.4f}")

"""# 문제 **2번**"""

import torch
from torch import nn
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torchvision import datasets
from torchvision.transforms import ToTensor

# 하이퍼파라미터
lr = 0.001
epochs = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1. 전체 데이터셋 불러오기 및 합치기
train_data_raw = datasets.FashionMNIST(root="data", train=True, download=True, transform=ToTensor())
test_data_raw = datasets.FashionMNIST(root="data", train=False, download=True, transform=ToTensor())
full_dataset = ConcatDataset([train_data_raw, test_data_raw])  # 전체 70,000개

# 2. 8:1:1로 나누기
total_size = len(full_dataset)  # 70,000
train_size = int(0.8 * total_size)     # 56,000
val_size = int(0.1 * total_size)       # 7,000
test_size = total_size - train_size - val_size  # 7,000
train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])

# 3. DataLoader 설정 (minibatch 사용 → batch_size = 64)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=64)
test_loader = DataLoader(test_data, batch_size=64)

# 4. MLP 모델 정의 (2 hidden layer)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model(x)

model = MLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=lr)


#학습하기전 에폭별 loss를 기록하기 위해 빈 리스트 하나 생성
train_losses = []

# 5. 학습 루프
for epoch in range(epochs):
    model.train()
    epoch_loss = 0
    for X_train, y_train in train_loader:
        X_train, y_train = X_train.to(device), y_train.to(device)
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item() * X_train.size(0)  # 전체 loss 누적 (평균 계산용)
    avg_train_loss = epoch_loss / len(train_loader.dataset)
    train_losses.append(avg_train_loss)

# 모든 에폭 끝난 후, train/validation/test 성능 평가
model.eval()
with torch.no_grad():
    # Train Accuracy 측정
    correct_train, total_train = 0, 0
    for X_train, y_train in train_loader:  # 미니 배치 처리
        X_train, y_train = X_train.to(device), y_train.to(device)
        pred_train = model(X_train)
        _, predicted = torch.max(pred_train, 1)
        total_train += y_train.size(0)
        correct_train += (predicted == y_train).sum().item()
    train_acc = correct_train / total_train
    train_loss = criterion(pred_train, y_train).item()

    # Validation Accuracy 측정
    correct_val, total_val = 0, 0
    for X_val, y_val in val_loader:  # 미니 배치 처리
        X_val, y_val = X_val.to(device), y_val.to(device)
        pred_val = model(X_val)
        _, predicted = torch.max(pred_val, 1)
        total_val += y_val.size(0)
        correct_val += (predicted == y_val).sum().item()
    val_acc = correct_val / total_val
    val_loss = criterion(pred_val, y_val).item()

    # Test Accuracy 측정
    correct_test, total_test = 0, 0
    for X_test, y_test in test_loader:  # 미니 배치 처리
        X_test, y_test = X_test.to(device), y_test.to(device)
        pred_test = model(X_test)
        _, predicted = torch.max(pred_test, 1)
        total_test += y_test.size(0)
        correct_test += (predicted == y_test).sum().item()
    test_acc = correct_test / total_test
    test_loss = criterion(pred_test, y_test).item()

# 최종 출력
print("\n== 최종 결과 ==")
print(f"Train Accuracy:     {train_acc*100:.2f}% | Loss: {train_loss:.4f}")
print(f"Validation Accuracy:{val_acc*100:.2f}% | Loss: {val_loss:.4f}")
print(f"Test Accuracy:      {test_acc*100:.2f}% | Loss: {test_loss:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(range(1, epochs + 1), train_losses, label="Train Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.legend()
plt.grid(True)
plt.show()

"""# **문제 3번 - 정규화**"""

import torch
from torch import nn
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torchvision import datasets
from torchvision.transforms import ToTensor
from torchvision import transforms

# 하이퍼파라미터
lr = 0.001
epochs = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1-1. 데이터 nomalization
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # 평균 0.5, 표준편차 0.5 → [-1, 1] 범위로 정규화
])

# 1. 전체 데이터셋 불러오기 및 합치기
train_data_raw = datasets.FashionMNIST(root="data", train=True, download=True, transform=transform)
test_data_raw = datasets.FashionMNIST(root="data", train=False, download=True, transform=transform)
full_dataset = ConcatDataset([train_data_raw, test_data_raw])  # 전체 70,000개



# 2. 8:1:1로 나누기
total_size = len(full_dataset)  # 70,000
train_size = int(0.8 * total_size)     # 56,000
val_size = int(0.1 * total_size)       # 7,000
test_size = total_size - train_size - val_size  # 7,000
train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])
# 3. DataLoader 설정 (minibatch 사용 → batch_size = 64)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=64)
test_loader = DataLoader(test_data, batch_size=64)

# 4. MLP 모델 정의 (2 hidden layer)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model(x)

model = MLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=lr)

# 학습 전 에폭별 loss 저장용 리스트
train_losses = []
test_losses = []

# 학습 루프
for epoch in range(epochs):
    # ----- 학습 -----
    model.train()
    epoch_loss = 0
    correct, total = 0, 0

    for X_train, y_train in train_loader:
        X_train, y_train = X_train.to(device), y_train.to(device)
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * X_train.size(0)

        # 정확도 계산
        _, predicted = torch.max(output, 1)
        total += y_train.size(0)
        correct += (predicted == y_train).sum().item()

    avg_train_loss = epoch_loss / len(train_loader.dataset)
    train_acc = correct / total
    train_losses.append(avg_train_loss)

    # ----- 검증 -----
    model.eval()
    test_loss = 0
    correct, total = 0, 0

    with torch.no_grad():
        for X_test, y_test in test_loader:
            X_test, y_test = X_test.to(device), y_test.to(device)
            output = model(X_test)
            loss = criterion(output, y_test)

            test_loss += loss.item() * X_test.size(0)

            # 정확도 계산
            _, predicted = torch.max(output, 1)
            total += y_test.size(0)
            correct += (predicted == y_test).sum().item()

    avg_test_loss = test_loss / len(test_loader.dataset)
    test_acc = correct / total
    test_losses.append(avg_test_loss)

    # ----- 출력 -----
    print(f"Epoch {epoch+1}/{epochs} | "
          f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | "
          f"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc*100:.2f}%")

# ----- loss 시각화 -----
plt.figure(figsize=(10,5))
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train vs Test Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# **문제4번 - Optimizer**"""

import torch
from torch import nn
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torchvision import datasets
from torchvision.transforms import ToTensor
from torchvision import transforms
import matplotlib.pyplot as plt

# 하이퍼파라미터
lr = 0.001
epochs = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1-1. 데이터 nomalization
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # 평균 0.5, 표준편차 0.5 → [-1, 1] 범위로 정규화
])

# 1. 전체 데이터셋 불러오기 및 합치기
train_data_raw = datasets.FashionMNIST(root="data", train=True, download=True, transform=transform)
test_data_raw = datasets.FashionMNIST(root="data", train=False, download=True, transform=transform)
full_dataset = ConcatDataset([train_data_raw, test_data_raw])  # 전체 70,000개

# 2. 8:1:1로 나누기
total_size = len(full_dataset)  # 70,000
train_size = int(0.8 * total_size)     # 56,000
val_size = int(0.1 * total_size)       # 7,000
test_size = total_size - train_size - val_size  # 7,000
train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])

# 3. DataLoader 설정 (minibatch 사용 → batch_size = 64)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=64)
test_loader = DataLoader(test_data, batch_size=64)

# 4. MLP 모델 정의 (2 hidden layer)
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model(x)

# 옵티마이저 리스트
optimizers_dict = {
    'SGD': lambda params: torch.optim.SGD(params, lr=lr),
    'SGD+Momentum': lambda params: torch.optim.SGD(params, lr=lr, momentum=0.9),
    'SGD+Nesterov': lambda params: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True),
    'AdaGrad': lambda params: torch.optim.Adagrad(params, lr=lr),
    'RMSProp': lambda params: torch.optim.RMSprop(params, lr=lr),
    'Adam': lambda params: torch.optim.Adam(params, lr=lr)
}

results = {}

# 옵티마이저 별로 순차적으로 학습
for name, opt_func in optimizers_dict.items():
    print(f"Training with {name}")
    model = MLP().to(device)
    optimizer = opt_func(model.parameters())
    criterion = nn.CrossEntropyLoss()
    train_losses = []

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)
        avg_loss = epoch_loss / len(train_loader.dataset)
        train_losses.append(avg_loss)
        print(f"Epoch {epoch+1} - Loss: {avg_loss:.4f}")

    results[name] = train_losses
    torch.cuda.empty_cache()  # GPU 메모리 비우기

# 그래프 그리기
plt.figure(figsize=(10, 6))
for name, losses in results.items():
    plt.plot(range(1, epochs + 1), losses, label=name)

plt.xlabel("Epoch")
plt.ylabel("Training Loss")
plt.title("Loss vs Epoch (Optimizer Comparison)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **문제 5번 - Dropout**"""

import torch
from torch import nn
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torchvision import datasets
from torchvision.transforms import ToTensor
from torchvision import transforms

# 하이퍼파라미터
lr = 0.001
epochs = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1-1. 데이터 nomalization
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # 평균 0.5, 표준편차 0.5 → [-1, 1] 범위로 정규화
])

# 1. 전체 데이터셋 불러오기 및 합치기
train_data_raw = datasets.FashionMNIST(root="data", train=True, download=True, transform=transform)
test_data_raw = datasets.FashionMNIST(root="data", train=False, download=True, transform=transform)
full_dataset = ConcatDataset([train_data_raw, test_data_raw])  # 전체 70,000개



# 2. 8:1:1로 나누기
total_size = len(full_dataset)  # 70,000
train_size = int(0.8 * total_size)     # 56,000
val_size = int(0.1 * total_size)       # 7,000
test_size = total_size - train_size - val_size  # 7,000
train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])
# 3. DataLoader 설정 (minibatch 사용 → batch_size = 64)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=64)
test_loader = DataLoader(test_data, batch_size=64)

# Dropout을 포함한 MLP 모델 정의
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model(x)

model = MLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

#학습하기전 에폭별 loss를 기록하기 위해 빈 리스트 하나 생성

# 학습 전 에폭별 loss 저장용 리스트
train_losses = []
test_losses = []

# 학습 루프
for epoch in range(epochs):
    # ----- 학습 -----
    model.train()
    epoch_loss = 0
    correct, total = 0, 0

    for X_train, y_train in train_loader:
        X_train, y_train = X_train.to(device), y_train.to(device)
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * X_train.size(0)

        # 정확도 계산
        _, predicted = torch.max(output, 1)
        total += y_train.size(0)
        correct += (predicted == y_train).sum().item()

    avg_train_loss = epoch_loss / len(train_loader.dataset)
    train_acc = correct / total
    train_losses.append(avg_train_loss)

    # ----- 검증 -----
    model.eval()
    test_loss = 0
    correct, total = 0, 0

    with torch.no_grad():
        for X_test, y_test in test_loader:
            X_test, y_test = X_test.to(device), y_test.to(device)
            output = model(X_test)
            loss = criterion(output, y_test)

            test_loss += loss.item() * X_test.size(0)

            # 정확도 계산
            _, predicted = torch.max(output, 1)
            total += y_test.size(0)
            correct += (predicted == y_test).sum().item()

    avg_test_loss = test_loss / len(test_loader.dataset)
    test_acc = correct / total
    test_losses.append(avg_test_loss)

    # ----- 출력 -----
    print(f"Epoch {epoch+1}/{epochs} | "
          f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | "
          f"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc*100:.2f}%")

# ----- loss 시각화 -----
plt.figure(figsize=(10,5))
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train vs Test Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# 문제 6번 - BatchNorm1d **적용**"""

import torch
from torch import nn
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torchvision import datasets
from torchvision.transforms import ToTensor
from torchvision import transforms

# 하이퍼파라미터
lr = 0.001
epochs = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1-1. 데이터 nomalization
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # 평균 0.5, 표준편차 0.5 → [-1, 1] 범위로 정규화
])

# 1. 전체 데이터셋 불러오기 및 합치기
train_data_raw = datasets.FashionMNIST(root="data", train=True, download=True, transform=transform)
test_data_raw = datasets.FashionMNIST(root="data", train=False, download=True, transform=transform)
full_dataset = ConcatDataset([train_data_raw, test_data_raw])  # 전체 70,000개



# 2. 8:1:1로 나누기
total_size = len(full_dataset)  # 70,000
train_size = int(0.8 * total_size)     # 56,000
val_size = int(0.1 * total_size)       # 7,000
test_size = total_size - train_size - val_size  # 7,000
train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])
# 3. DataLoader 설정 (minibatch 사용 → batch_size = 64)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=64)
test_loader = DataLoader(test_data, batch_size=64)

# Dropout + BatchNorm 포함한 MLP 정의
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.BatchNorm1d(512),  # BatchNorm 추가
            nn.ReLU(),
            nn.Dropout(0.5),

            nn.Linear(512, 256),
            nn.BatchNorm1d(256),  # BatchNorm 추가
            nn.ReLU(),
            nn.Dropout(0.5),

            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model(x)

model = MLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

#학습하기전 에폭별 loss를 기록하기 위해 빈 리스트 하나 생성

# 학습 전 에폭별 loss 저장용 리스트
train_losses = []
test_losses = []

# 학습 루프
for epoch in range(epochs):
    # ----- 학습 -----
    model.train()
    epoch_loss = 0
    correct, total = 0, 0

    for X_train, y_train in train_loader:
        X_train, y_train = X_train.to(device), y_train.to(device)
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * X_train.size(0)

        # 정확도 계산
        _, predicted = torch.max(output, 1)
        total += y_train.size(0)
        correct += (predicted == y_train).sum().item()

    avg_train_loss = epoch_loss / len(train_loader.dataset)
    train_acc = correct / total
    train_losses.append(avg_train_loss)

    # ----- 검증 -----
    model.eval()
    test_loss = 0
    correct, total = 0, 0

    with torch.no_grad():
        for X_test, y_test in test_loader:
            X_test, y_test = X_test.to(device), y_test.to(device)
            output = model(X_test)
            loss = criterion(output, y_test)

            test_loss += loss.item() * X_test.size(0)

            # 정확도 계산
            _, predicted = torch.max(output, 1)
            total += y_test.size(0)
            correct += (predicted == y_test).sum().item()

    avg_test_loss = test_loss / len(test_loader.dataset)
    test_acc = correct / total
    test_losses.append(avg_test_loss)

    # ----- 출력 -----
    print(f"Epoch {epoch+1}/{epochs} | "
          f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | "
          f"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_acc*100:.2f}%")

# ----- loss 시각화 -----
plt.figure(figsize=(10,5))
plt.plot(train_losses, label='Train Loss')
plt.plot(test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train vs Test Loss')
plt.legend()
plt.grid(True)
plt.show()

"""# **문제7번 - hyperparameter 수정**"""

import torch
from torch import nn
from torch.utils.data import DataLoader, ConcatDataset, random_split
from torchvision import datasets
from torchvision.transforms import ToTensor
from torchvision import transforms

# 하이퍼파라미터
lr = 0.001
epochs = 20
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 1-1. 데이터 nomalization
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # 평균 0.5, 표준편차 0.5 → [-1, 1] 범위로 정규화
])

# 1. 전체 데이터셋 불러오기 및 합치기
train_data_raw = datasets.FashionMNIST(root="data", train=True, download=True, transform=transform)
test_data_raw = datasets.FashionMNIST(root="data", train=False, download=True, transform=transform)
full_dataset = ConcatDataset([train_data_raw, test_data_raw])  # 전체 70,000개



# 2. 8:1:1로 나누기
total_size = len(full_dataset)  # 70,000
train_size = int(0.8 * total_size)     # 56,000
val_size = int(0.1 * total_size)       # 7,000
test_size = total_size - train_size - val_size  # 7,000
train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])
# 3. DataLoader 설정 (minibatch 사용 → batch_size = 64)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
val_loader = DataLoader(val_data, batch_size=64)
test_loader = DataLoader(test_data, batch_size=64)

# Dropout + BatchNorm 포함한 MLP 정의
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.BatchNorm1d(512),  # BatchNorm 추가
            nn.ReLU(),
            nn.Dropout(0.5),

            nn.Linear(512, 256),
            nn.BatchNorm1d(256),  # BatchNorm 추가
            nn.ReLU(),
            nn.Dropout(0.5),

            nn.Linear(256, 10)
        )

    def forward(self, x):
        return self.model(x)

model = MLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

#학습하기전 에폭별 loss를 기록하기 위해 빈 리스트 하나 생성

# 학습 전 에폭별 loss 저장용 리스트
train_losses = []

# 학습 루프 (validation/test는 제외)
for epoch in range(epochs):
    model.train()
    epoch_loss = 0
    correct, total = 0, 0

    for X_train, y_train in train_loader:
        X_train, y_train = X_train.to(device), y_train.to(device)
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * X_train.size(0)
        _, predicted = torch.max(output, 1)
        total += y_train.size(0)
        correct += (predicted == y_train).sum().item()

    avg_train_loss = epoch_loss / len(train_loader.dataset)
    train_acc = correct / total
    train_losses.append(avg_train_loss)

    print(f"Epoch {epoch+1}/{epochs} | "
          f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc*100:.2f}%")

# ----- validation + test는 한 번만 수행 -----
model.eval()
with torch.no_grad():
    # Validation
    correct, total, val_loss = 0, 0, 0
    for X_val, y_val in val_loader:
        X_val, y_val = X_val.to(device), y_val.to(device)
        output = model(X_val)
        val_loss += criterion(output, y_val).item() * X_val.size(0)
        _, predicted = torch.max(output, 1)
        total += y_val.size(0)
        correct += (predicted == y_val).sum().item()
    avg_val_loss = val_loss / len(val_loader.dataset)
    val_acc = correct / total

    # Test
    correct, total, test_loss = 0, 0, 0
    for X_test, y_test in test_loader:
        X_test, y_test = X_test.to(device), y_test.to(device)
        output = model(X_test)
        test_loss += criterion(output, y_test).item() * X_test.size(0)
        _, predicted = torch.max(output, 1)
        total += y_test.size(0)
        correct += (predicted == y_test).sum().item()
    avg_test_loss = test_loss / len(test_loader.dataset)
    test_acc = correct / total

# 최종 출력
print("\n== 최종 결과 ==")
print(f"Validation Accuracy: {val_acc*100:.2f}% | Loss: {avg_val_loss:.4f}")
print(f"Test Accuracy:      {test_acc*100:.2f}% | Loss: {avg_test_loss:.4f}")