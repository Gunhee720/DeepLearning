# -*- coding: utf-8 -*-
"""DL_HW4_60192328_강건희 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xSJVuQFryFDeQCb1HvclIlzJcUUQQvrx
"""

!pip install torchtext==0.17.0 --force-reinstall
!pip install transformers==4.38.2
!pip install accelerate==0.27.2

!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xzf aclImdb_v1.tar.gz

"""# **실습 코드를 기반으로 똑같이 LSTM 기반 학습 코드를 구현하여 10 epoch training에 대한 train / test 결과를 확인하세요. (20)**"""

import numpy as np
import torch
seed = 1234
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True

import os
import pandas as pd
def load_imdb_split(split_dir):
    texts, labels = [], []
    for label in ['pos', 'neg']:
      folder = os.path.join(split_dir, label)
      for filename in os.listdir(folder):
        with open(os.path.join(folder, filename), 'r', encoding='utf-8') as f:
          texts.append(f.read())
          labels.append(1 if label == 'pos' else 0)
    return pd.DataFrame({'text': texts, 'label': labels})
train_df = load_imdb_split('aclImdb/train')
test_df = load_imdb_split('aclImdb/test')

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

tokenizer = get_tokenizer("basic_english")
def yield_tokens(text_series):
    for text in text_series:
       yield tokenizer(text)

# Build vocab using training data only
vocab = build_vocab_from_iterator(yield_tokens(train_df['text']), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

MAX_LEN=256
def encode_text(text):
    tokens = tokenizer(text)
    return vocab(tokens[:MAX_LEN])

train_df['input_ids'] = train_df['text'].apply(encode_text)
test_df['input_ids'] = test_df['text'].apply(encode_text)

import torch
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import TensorDataset, DataLoader
def create_tensor_dataset(inputs, labels):
    tensor_seqs = [torch.tensor(x) for x in inputs]
    padded_seqs = pad_sequence(tensor_seqs, batch_first=True)
    labels_tensor = torch.tensor(list(labels), dtype=torch.long)

    return TensorDataset(padded_seqs, labels_tensor)

train_dataset = create_tensor_dataset(train_df['input_ids'], train_df['label'])
test_dataset = create_tensor_dataset(test_df['input_ids'], test_df['label'])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

import torch.nn as nn
class SimpleLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, output_dim=1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim,
        padding_idx=vocab["<unk>"])
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        embedded = self.embedding(x) # (batch, seq_len, embedding_dim)
        output, (hidden, _) = self.lstm(embedded) # hidden: (1, batch, hidden_dim)
        hidden = hidden.squeeze(0) # (batch, hidden_dim)
        return self.sigmoid(self.fc(hidden))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleLSTMClassifier(vocab_size=len(vocab), embedding_dim = 300, hidden_dim =
128).to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)


from tqdm import tqdm
def train_loop(model, train_loader, test_loader, optimizer, criterion, device, epochs=5):
    model.to(device)
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0
        total_correct = 0
        print(f"\n Epoch {epoch}/{epochs}")
        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc="Training")
        for batch_idx, (x_batch, y_batch) in progress_bar:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device).float()
            optimizer.zero_grad()
            outputs = model(x_batch).squeeze(1)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            predictions = (outputs >= 0.5).long()
            total_correct += (predictions == y_batch).sum().item()
            progress_bar.set_postfix(loss=loss.item())
        train_acc = total_correct / len(train_loader.dataset)
        avg_loss = total_loss / len(train_loader)
        model.eval()
        with torch.no_grad():
            correct = 0
            for x_batch, y_batch in test_loader:
                x_batch, y_batch = x_batch.to(device), y_batch.to(device).float()
                outputs = model(x_batch).squeeze(1)
                preds = (outputs >= 0.5).long()
                correct += (preds == y_batch).sum().item()
            test_acc = correct / len(test_loader.dataset)

        print(f"Epoch {epoch} Summary - Avg Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")
    return model

model = train_loop(
model=model,
train_loader=train_loader,
test_loader=test_loader,
optimizer=optimizer,
criterion=criterion,
device=device,
epochs=10
)

"""# **epoch, learning_rate, batch_size를 고정한채 LSTM 기반 모델의 test set 성능을 87% 이상으로 올려보고 어떻게 성능을 향상시켰는지 작성하세요. (20) +Glove (6B, 300dim) pretrained word embedding 모델을 적용하여 성능을 확인하세요. (20)**"""

import torchtext
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleLSTMClassifier(vocab_size=len(vocab), embedding_dim = 300, hidden_dim =
128).to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
vectors = torchtext.vocab.GloVe(name='6B', dim=300,cache='~/.vector_cache')
pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())
model.embedding.weight.data = pretrained_embedding

model.embedding.weight.requires_grad = False

import torch.nn as nn

def init_weights(m):
    if isinstance(m, nn.Linear) or isinstance(m, nn.Embedding):
        nn.init.xavier_uniform_(m.weight)  # 예: Xavier 초기화
        if hasattr(m, 'bias') and m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LSTM):
        for name, param in m.named_parameters():
            if 'weight_ih' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'weight_hh' in name:
                nn.init.orthogonal_(param.data)
            elif 'bias' in name:
                nn.init.zeros_(param.data)

# 모델에 초기화 적용
naive_transformer_model.apply(init_weights)

model = train_loop(
model=model,
train_loader=train_loader,
test_loader=test_loader,
optimizer=optimizer,
criterion=criterion,
device=device,
epochs=10
)

"""# **실습코드를 활용하여 naïve_transformer_model을 training 하려면 되지 않는다. training 되도록 코드를 수정하세요. (20)**
**– training이 되도록 고치면 성능이 형편없다. epoch, learning_rate, batch_size를 고정한채 naïve_taransformer_model 기반 모델의 test set**
성능을 82% 이상으로 올려보고 어떻게 성능을 향상 시켰는지 작성하세요. (20)

"""

import torch
import torch.nn as nn
import math
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=256):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) /
    d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.pe = pe.unsqueeze(0) # (1, max_len, d_model)
    def forward(self, x):
       return x + self.pe[:, :x.size(1)].to(x.device)
class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim=300, num_heads=4, hidden_dim=128,
                 num_layers=2, num_classes=1, max_len=256, dropout=0.1, pad_idx=0):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)
        self.pos_encoder = PositionalEncoding(embedding_dim, max_len)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=hidden_dim,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.classifier = nn.Linear(embedding_dim, 1)

    def forward(self, x):
        embedded = self.embedding(x)  # (batch, seq_len, emb_dim)
        encoded = self.pos_encoder(embedded)
        transformer_out = self.transformer_encoder(encoded)  # (batch, seq_len, emb_dim)
        pooled = transformer_out.mean(dim=1)  # mean pooling
        # return self.classifier(pooled).squeeze(1)  # (batch,)
        return torch.sigmoid(self.classifier(pooled))  # .squeeze(1) 제거, sigmoid 적용

naive_transformer_model = TransformerClassifier(
    vocab_size=len(vocab),
    embedding_dim=300,
    # num_heads=8,
    num_heads=4,
    hidden_dim=256,
    # num_layers=6,
    num_layers=2,
    pad_idx=vocab["<unk>"]
).to(device)

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(naive_transformer_model.parameters(), lr=1e-3)

naive_transformer_model = train_loop(
    model=naive_transformer_model,
    train_loader=train_loader,
    test_loader=test_loader,
    optimizer=optimizer,
    criterion=criterion,
    device=device,
    epochs=10
)

"""## **다른 ipynb에 있던 코드라 결과화면이 없습니다_ 보고서의 있는 캡쳐사진처럼 결과가 나왔습니다.
# !pip install torchtext==0.17.0 --force-reinstall!pip install transformers==4.38.2 !pip install accelerate==0.27.2 전에 실행해야 오류가 안나서 제일 처음으로 실행했습니다.

*bert – 위 방법 외에 어떤 방법을 쓰던 sentiment classification을 하여 test set 성능을 높이기 위한 모델을 개발하고 본인의 최종 모델은 무엇이고 성능은 얼마이며 성능을 올리기 위해 어떤 것들을 수행하였는지 정리하세요. (20) - **
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from tqdm import tqdm
from transformers import BertTokenizer, BertModel

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from tqdm import tqdm
from transformers import BertTokenizer, BertModel

import numpy as np
import torch
def load_imdb_split(split_dir):
    texts, labels = [], []
    for label in ['pos', 'neg']:
      folder = os.path.join(split_dir, label)
      for filename in os.listdir(folder):
        with open(os.path.join(folder, filename), 'r', encoding='utf-8') as f:
          texts.append(f.read())
          labels.append(1 if label == 'pos' else 0)
    return pd.DataFrame({'text': texts, 'label': labels})
train_df = load_imdb_split('aclImdb/train')
test_df = load_imdb_split('aclImdb/test')

train_df = load_imdb_split('aclImdb/train')
test_df = load_imdb_split('aclImdb/test')

# 2. BERT 토크나이저
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# 3. Dataset 클래스 정의
class IMDBDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            padding='max_length',
            truncation=True,
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        label = torch.tensor(self.labels[idx], dtype=torch.float)
        return input_ids, attention_mask, label

    def __len__(self):
        return len(self.texts)

# 4. DataLoader 준비
train_dataset = IMDBDataset(train_df['text'].tolist(), train_df['label'].tolist(), tokenizer)
test_dataset = IMDBDataset(test_df['text'].tolist(), test_df['label'].tolist(), tokenizer)


train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)
#다른 모델 "distilbert-base-uncased"
# 5. BERT 기반 분류 모델 정의
class BertClassifier(nn.Module):
    def __init__(self, dropout=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(768, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state.mean(dim=1)
        logits = self.classifier(self.dropout(pooled))
        return logits.squeeze(1)

# 6. 학습 함수
def train_loop(model, train_loader, test_loader, optimizer, criterion, device, epochs=10):
    model.to(device)
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0
        total_correct = 0
        print(f"\nEpoch {epoch}/{epochs}")
        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc="Training")

        for batch_idx, (input_ids, attention_mask, y_batch) in progress_bar:
            input_ids, attention_mask, y_batch = input_ids.to(device), attention_mask.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            predictions = (torch.sigmoid(outputs) >= 0.5).long()
            total_correct += (predictions == y_batch.long()).sum().item()
            progress_bar.set_postfix(loss=loss.item())

        train_acc = total_correct / len(train_loader.dataset)
        avg_loss = total_loss / len(train_loader)

        # 평가
        model.eval()
        with torch.no_grad():
            correct = 0
            for input_ids, attention_mask, y_batch in test_loader:
                input_ids, attention_mask, y_batch = input_ids.to(device), attention_mask.to(device), y_batch.to(device)
                outputs = model(input_ids, attention_mask)
                preds = (torch.sigmoid(outputs) >= 0.5).long()
                correct += (preds == y_batch.long()).sum().item()

            test_acc = correct / len(test_loader.dataset)

        print(f"Epoch {epoch} Summary - Avg Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}")

    return model

# 7. 학습 실행
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = BertClassifier(dropout=0.3)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)
    mymodel=train_loop(model, train_loader, test_loader, optimizer, criterion, device, epochs=3)