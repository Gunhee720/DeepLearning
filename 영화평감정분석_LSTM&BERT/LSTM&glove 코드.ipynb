{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5ENXYWMtN2I",
        "outputId": "1d4808ba-5b0d-43fb-feef-f1db85507cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-16 08:34:32--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  36.6MB/s    in 2.2s    \n",
            "\n",
            "2025-06-16 08:34:34 (36.6 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX3Nx_x3Qh8L"
      },
      "source": [
        "# **실습 코드를 기반으로 똑같이 LSTM 기반 학습 코드를 구현하여 10 epoch training에 대한 train / test 결과를 확인하세요. (20)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx5WEf0NprP9",
        "outputId": "866ae480-d742-47ea-f214-57ed19f3ba1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:09<00:00, 43.23it/s, loss=0.706]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Summary - Avg Loss: 0.6932, Train Acc: 0.5064, Test Acc: 0.5038\n",
            "\n",
            " Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 47.82it/s, loss=0.676]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Summary - Avg Loss: 0.6579, Train Acc: 0.6098, Test Acc: 0.5276\n",
            "\n",
            " Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 47.91it/s, loss=0.515]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Summary - Avg Loss: 0.6119, Train Acc: 0.6470, Test Acc: 0.6026\n",
            "\n",
            " Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 46.84it/s, loss=0.574]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Summary - Avg Loss: 0.5527, Train Acc: 0.7104, Test Acc: 0.5299\n",
            "\n",
            " Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 47.07it/s, loss=0.49]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Summary - Avg Loss: 0.5075, Train Acc: 0.7400, Test Acc: 0.6811\n",
            "\n",
            " Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 46.67it/s, loss=0.562]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Summary - Avg Loss: 0.4660, Train Acc: 0.7675, Test Acc: 0.7085\n",
            "\n",
            " Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 47.01it/s, loss=0.291]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Summary - Avg Loss: 0.4576, Train Acc: 0.7524, Test Acc: 0.7157\n",
            "\n",
            " Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 47.02it/s, loss=0.277]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Summary - Avg Loss: 0.3553, Train Acc: 0.8464, Test Acc: 0.7652\n",
            "\n",
            " Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 46.64it/s, loss=0.445]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 Summary - Avg Loss: 0.3093, Train Acc: 0.8748, Test Acc: 0.7005\n",
            "\n",
            " Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:08<00:00, 46.36it/s, loss=0.346]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 Summary - Avg Loss: 0.2594, Train Acc: 0.9032, Test Acc: 0.7810\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "def load_imdb_split(split_dir):\n",
        "    texts, labels = [], []\n",
        "    for label in ['pos', 'neg']:\n",
        "      folder = os.path.join(split_dir, label)\n",
        "      for filename in os.listdir(folder):\n",
        "        with open(os.path.join(folder, filename), 'r', encoding='utf-8') as f:\n",
        "          texts.append(f.read())\n",
        "          labels.append(1 if label == 'pos' else 0)\n",
        "    return pd.DataFrame({'text': texts, 'label': labels})\n",
        "train_df = load_imdb_split('aclImdb/train')\n",
        "test_df = load_imdb_split('aclImdb/test')\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(text_series):\n",
        "    for text in text_series:\n",
        "       yield tokenizer(text)\n",
        "\n",
        "# Build vocab using training data only\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_df['text']), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "MAX_LEN=256\n",
        "def encode_text(text):\n",
        "    tokens = tokenizer(text)\n",
        "    return vocab(tokens[:MAX_LEN])\n",
        "\n",
        "train_df['input_ids'] = train_df['text'].apply(encode_text)\n",
        "test_df['input_ids'] = test_df['text'].apply(encode_text)\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "def create_tensor_dataset(inputs, labels):\n",
        "    tensor_seqs = [torch.tensor(x) for x in inputs]\n",
        "    padded_seqs = pad_sequence(tensor_seqs, batch_first=True)\n",
        "    labels_tensor = torch.tensor(list(labels), dtype=torch.long)\n",
        "\n",
        "    return TensorDataset(padded_seqs, labels_tensor)\n",
        "\n",
        "train_dataset = create_tensor_dataset(train_df['input_ids'], train_df['label'])\n",
        "test_dataset = create_tensor_dataset(test_df['input_ids'], test_df['label'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "import torch.nn as nn\n",
        "class SimpleLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim,\n",
        "        padding_idx=vocab[\"<unk>\"])\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x) # (batch, seq_len, embedding_dim)\n",
        "        output, (hidden, _) = self.lstm(embedded) # hidden: (1, batch, hidden_dim)\n",
        "        hidden = hidden.squeeze(0) # (batch, hidden_dim)\n",
        "        return self.sigmoid(self.fc(hidden))\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleLSTMClassifier(vocab_size=len(vocab), embedding_dim = 300, hidden_dim =\n",
        "128).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "def train_loop(model, train_loader, test_loader, optimizer, criterion, device, epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        print(f\"\\n Epoch {epoch}/{epochs}\")\n",
        "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
        "        for batch_idx, (x_batch, y_batch) in progress_bar:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x_batch).squeeze(1)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            predictions = (outputs >= 0.5).long()\n",
        "            total_correct += (predictions == y_batch).sum().item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "        train_acc = total_correct / len(train_loader.dataset)\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            for x_batch, y_batch in test_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device).float()\n",
        "                outputs = model(x_batch).squeeze(1)\n",
        "                preds = (outputs >= 0.5).long()\n",
        "                correct += (preds == y_batch).sum().item()\n",
        "            test_acc = correct / len(test_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch} Summary - Avg Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "model = train_loop(\n",
        "model=model,\n",
        "train_loader=train_loader,\n",
        "test_loader=test_loader,\n",
        "optimizer=optimizer,\n",
        "criterion=criterion,\n",
        "device=device,\n",
        "epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nHGhnli2ubrH"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleLSTMClassifier(vocab_size=len(vocab), embedding_dim = 300, hidden_dim =\n",
        "128).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "vectors = torchtext.vocab.GloVe(name='6B', dim=300,cache='~/.vector_cache')\n",
        "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())\n",
        "model.embedding.weight.data = pretrained_embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2Mbeh7kuyXE8"
      },
      "outputs": [],
      "source": [
        "model.embedding.weight.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHz40UkDygyZ",
        "outputId": "8ea891f3-ed71-44bb-af2f-28e40923116f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (embedding): Embedding(100683, 300, padding_idx=0)\n",
              "  (pos_encoder): PositionalEncoding()\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=300, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=300, bias=True)\n",
              "        (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=300, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Embedding):\n",
        "        nn.init.xavier_uniform_(m.weight)  # 예: Xavier 초기화\n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param.data)\n",
        "\n",
        "# 모델에 초기화 적용\n",
        "naive_transformer_model.apply(init_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuxewr20QuNl"
      },
      "source": [
        "# **epoch, learning_rate, batch_size를 고정한채 LSTM 기반 모델의 test set 성능을 87% 이상으로 올려보고 어떻게 성능을 향상시켰는지 작성하세요. (20) +Glove (6B, 300dim) pretrained word embedding 모델을 적용하여 성능을 확인하세요. (20)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_iwtOajxTA2",
        "outputId": "b50701a6-98b2-4b31-c400-68e39df7e73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 83.49it/s, loss=0.685]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Summary - Avg Loss: 0.6868, Train Acc: 0.5282, Test Acc: 0.5090\n",
            "\n",
            " Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 83.67it/s, loss=0.64]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Summary - Avg Loss: 0.6722, Train Acc: 0.5817, Test Acc: 0.6434\n",
            "\n",
            " Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 82.54it/s, loss=0.586]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Summary - Avg Loss: 0.6148, Train Acc: 0.6869, Test Acc: 0.7507\n",
            "\n",
            " Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 80.66it/s, loss=0.608]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Summary - Avg Loss: 0.6000, Train Acc: 0.6981, Test Acc: 0.7208\n",
            "\n",
            " Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 82.58it/s, loss=0.313]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Summary - Avg Loss: 0.4310, Train Acc: 0.8100, Test Acc: 0.8476\n",
            "\n",
            " Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 81.96it/s, loss=0.442]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Summary - Avg Loss: 0.3340, Train Acc: 0.8591, Test Acc: 0.8600\n",
            "\n",
            " Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 83.56it/s, loss=0.358]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Summary - Avg Loss: 0.3058, Train Acc: 0.8713, Test Acc: 0.8686\n",
            "\n",
            " Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 83.20it/s, loss=0.292]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Summary - Avg Loss: 0.2835, Train Acc: 0.8819, Test Acc: 0.8684\n",
            "\n",
            " Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 84.81it/s, loss=0.0967]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 Summary - Avg Loss: 0.2609, Train Acc: 0.8922, Test Acc: 0.8731\n",
            "\n",
            " Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:04<00:00, 83.83it/s, loss=0.129]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 Summary - Avg Loss: 0.2378, Train Acc: 0.9045, Test Acc: 0.8726\n"
          ]
        }
      ],
      "source": [
        "model = train_loop(\n",
        "model=model,\n",
        "train_loader=train_loader,\n",
        "test_loader=test_loader,\n",
        "optimizer=optimizer,\n",
        "criterion=criterion,\n",
        "device=device,\n",
        "epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "yZi5dRRmz90n",
        "outputId": "3495dbf8-3cf5-4693-c4f9-e5325e67b261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [01:01<00:00,  6.35it/s, loss=0.685]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Summary - Avg Loss: 0.6953, Train Acc: 0.5003, Test Acc: 0.5013\n",
            "\n",
            " Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  29%|██▉       | 113/391 [00:17<00:43,  6.37it/s, loss=0.711]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1683276462>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m ).to(device)\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m naive_transformer_model = train_loop(\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnaive_transformer_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-1683276462>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train_loader, test_loader, optimizer, criterion, device, epochs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=300, num_heads=4, hidden_dim=128,\n",
        "                 num_layers=2, num_classes=1, max_len=256, dropout=0.1, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, max_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, emb_dim)\n",
        "        encoded = self.pos_encoder(embedded)\n",
        "        transformer_out = self.transformer_encoder(encoded)  # (batch, seq_len, emb_dim)\n",
        "        pooled = transformer_out.mean(dim=1)  # mean pooling\n",
        "        # return self.classifier(pooled).squeeze(1)  # (batch,)\n",
        "        return torch.sigmoid(self.classifier(pooled))  # .squeeze(1) 제거, sigmoid 적용\n",
        "\n",
        "naive_transformer_model = TransformerClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=300,\n",
        "    # num_heads=8,\n",
        "    num_heads=4,\n",
        "    hidden_dim=256,\n",
        "    # num_layers=6,\n",
        "    num_layers=2,\n",
        "    pad_idx=vocab[\"<unk>\"]\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(naive_transformer_model.parameters(), lr=1e-3)\n",
        "\n",
        "naive_transformer_model = train_loop(\n",
        "    model=naive_transformer_model,\n",
        "    train_loader=train_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqJRvGd9Gvu9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPyTvfH7Gviw"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
