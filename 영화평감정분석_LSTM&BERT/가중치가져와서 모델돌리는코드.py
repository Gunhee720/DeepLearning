# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NkDXs7dSJ7qCHNJ2Jizbqlnr4dlJ_t8h
"""

# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RpeCaqFEbAUkgmdH_YDB_4n8Yzc7rF2c
"""

import os
import tarfile 
os.system("wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz")

with tarfile.open("aclImdb_v1.tar.gz", "r:gz") as tar:
    tar.extractall()


import torch
from transformers import BertModel
import torch.nn as nn
from torch.utils.data import DataLoader
import os
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from tqdm import tqdm
from transformers import BertTokenizer, BertModel
import numpy as np
import torch

seed = 1234
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True


# 1. 데이터 로딩 (IMDB)
def load_imdb_split(split_dir):
    texts, labels = [], []
    for label in ['pos', 'neg']:
        folder = os.path.join(split_dir, label)
        for filename in os.listdir(folder):
            with open(os.path.join(folder, filename), 'r', encoding='utf-8') as f:
                texts.append(f.read())
                labels.append(1 if label == 'pos' else 0)
    return pd.DataFrame({'text': texts, 'label': labels})

train_df = load_imdb_split('aclImdb/train')
test_df = load_imdb_split('aclImdb/test')

# 2. BERT 토크나이저
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# 3. Dataset 클래스 정의
class IMDBDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            padding='max_length',
            truncation=True,
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        label = torch.tensor(self.labels[idx], dtype=torch.float)
        return input_ids, attention_mask, label

    def __len__(self):
        return len(self.texts)

# 4. DataLoader 준비
train_dataset = IMDBDataset(train_df['text'].tolist(), train_df['label'].tolist(), tokenizer)
test_dataset = IMDBDataset(test_df['text'].tolist(), test_df['label'].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)
# 1. 모델 구조 다시 정의
class BertClassifier(nn.Module):
    def __init__(self, dropout=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(768, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state.mean(dim=1)
        logits = self.classifier(self.dropout(pooled))
        return logits.squeeze(1)
model_path = f"{data_dir}/{model_name}.pt"

# 2. 모델 인스턴스 생성 및 가중치 로드
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertClassifier()
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()

# 3. 정확도 평가 (test_loader는 기존과 동일하게 존재한다고 가정)
correct = 0
total = 0
results = []

with torch.no_grad():
    for idx, (input_ids, attention_mask, y_batch) in enumerate(test_loader):
        input_ids, attention_mask, y_batch = input_ids.to(device), attention_mask.to(device), y_batch.to(device)

        # 모델 출력
        outputs = model(input_ids, attention_mask)

        # 예측 확률을 sigmoid 함수로 변환 후, 0.5 이상이면 1 (positive), 그 외는 0 (negative)
        preds = (torch.sigmoid(outputs) >= 0.5).long()

        # 정확도 계산
        correct += (preds == y_batch.long()).sum().item()
        total += y_batch.size(0)

        # 문장 index와 예측된 결과 저장
        for i in range(len(y_batch)):
            sentence_idx = idx * len(y_batch) + i  # batch 내에서 문장의 인덱스
            label = "positive" if preds[i] == 1 else "negative"  # 긍정/부정 라벨
            results.append([sentence_idx, label])

# 정확도 출력
accuracy = correct / total
print(f"Test Accuracy: {accuracy:.4f}")

# 4. 결과를 result.tsv 파일로 저장
result_df = pd.DataFrame(results, columns=["Index", "Prediction"])
result_df.to_csv("result.tsv", sep="\t", index=False)


print("Results have been saved to result.tsv")